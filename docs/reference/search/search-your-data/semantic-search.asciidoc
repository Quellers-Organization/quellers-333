[[semantic-search]]
== Semantic search

Semantic search is a search method that helps you find data based on the intent and contextual meaning of a search query, instead of a match on query terms (lexical search).

{es} provides various semantic search capabilities using {ml-docs}/ml-nlp.html[natural language processing (NLP)] and vector search.
Using an NLP model enables you to extract text embeddings out of text.
Embeddings are vectors that provide a numeric representation of a text.
Pieces of content with similar meaning have similar representations.
NLP models can be used in the {stack} various ways, you can:

* use the <<semantic-search-semantic-text, `semantic_text` workflow>> (recommended)
* use the <<semantic-search-inference, {infer} API workflow>>
* deploy models directly in {es}


[[semantic-search-diagram]]
.A simplified representation of encoding textual concepts as vectors
image::images/search/vector-search-oversimplification.png[A simplified representation of encoding textual concepts as vectors,align="center"]

At query time, {es} can use the same NLP model to convert a query into embeddings, enabling you to find documents with similar text embeddings.


[discrete]
[[using-nlp-models]]
=== Using NLP models

The easiest and recommended way to use NLP models in the {stack} is the <<semantic-search-semantic-text, `semantic_text` workflow>>.
If you want to use ELSER for semantic search or you already have a service you use, create an {infer} endpoint and an index mapping to start ingesting and querying data.
You don't need to define model-related settings and parameters, or create {infer} ingest pipelines.
Refer to the <<put-inference-api, Create an {infer} endpoint API>> documentation for a list of supported services.

The <<semantic-search-inference, {infer} API workflow>> more complex but enables you to have more control over the {infer} endpoint configuration.
You need to create an {infer} endpoint and provide various model-related settings and parameters, define an index mapping, and set up an {infer} ingest pipeline with the correct settings.

You can also deploy NLP models directly in {es}.
This is the most complex way to perform semantic search in the {stack}.
You need to select an NLP model from the {ml-docs}/ml-nlp-model-ref.html#ml-nlp-model-ref-text-embedding[list of supported NLP models] that includes both dense and sparse vector models.
Then deploy the selected model by using the Eland client, create an index mapping and a sufficient ingest pipeline to start ingesting and querying data.


[discrete]
[[using-query]]
=== Using the right query

Crafting the right query is crucial for semantic search.
The query type you should use depends first on whether you are using the recommended `semantic_text` workflow.
If not, it depends on the vector type in which your embeddings are stored.

[cols="3*", options="header"]
|=======================================================================================================================================================================================================
| Field to query                    | Query to use                                      | Notes                                                                                                                                                             
| <<semantic-text,`semantic_text`>> | <<query-dsl-semantic-query,`semantic`>> .         | The `semantic_text` field handles generating embeddings for you at index time and query time.                                                                     
| <<sparse-vector,`sparse_vector`>> | <<query-dsl-sparse-vector-query,`sparse vector`>> | The `sparse_vector` query can generate query embeddings for you, but you can also provide your own. You are expected to provide embeddings at index time.
| <<dense-vector,`dense_vector`>>   | <<query-dsl-knn-query,`knn`>>                     | The `knn` query can generate query embeddings for you, but you can also provide your own. You are expected to provide embeddings at index time.
|=======================================================================================================================================================================================================

If you want {es} to generate embeddings for you both index and query time, use the `semantic_text` field and the `semantic` query.
If you want to bring your own embeddings, store them in {es} and use the `sparse_vector` or `dense_vector` field type and the associated query depending on the NLP model you used for generating the embeddings.

IMPORTANT: For the easiest way to perform semantic search in the {stack}, refer to the <<semantic-search-semantic-text, `semantic_text`>> end-to-end tutorial.


[discrete]
[[semantic-search-read-more]]
=== Read more

* Tutorials:
** <<semantic-search-semantic-text, Semantic search with `semantic_text`>>
** <<semantic-search-inference, Semantic search with the {infer} API>>
** <<semantic-search-elser,Semantic search with ELSER>> using the {infer} workflow
** <<semantic-search-deployed-nlp-model, Semantic search with a model deployed in {es}>>
** {ml-docs}/ml-nlp-text-emb-vector-search-example.html[Semantic search with the msmarco-MiniLM-L-12-v3 sentence-transformer model]
* Interactive examples:
** The https://github.com/elastic/elasticsearch-labs[`elasticsearch-labs`] repo contains a number of interactive semantic search examples in the form of executable Python notebooks, using the {es} Python client
* Blogs:
** {blog-ref}may-2023-launch-sparse-encoder-ai-model[Introducing Elastic Learned Sparse Encoder: Elastic's AI model for semantic search]
** {blog-ref}lexical-ai-powered-search-elastic-vector-database[How to get the best of lexical and AI-powered search with Elastic's vector database]
** Information retrieval blog series:
*** {blog-ref}improving-information-retrieval-elastic-stack-search-relevance[Part 1: Steps to improve search relevance]
*** {blog-ref}improving-information-retrieval-elastic-stack-benchmarking-passage-retrieval[Part 2: Benchmarking passage retrieval]
*** {blog-ref}may-2023-launch-information-retrieval-elasticsearch-ai-model[Part 3: Introducing Elastic Learned Sparse Encoder, our new retrieval model]
*** {blog-ref}improving-information-retrieval-elastic-stack-hybrid[Part 4: Hybrid retrieval]


include::semantic-search-semantic-text.asciidoc[]
include::semantic-search-inference.asciidoc[]
include::semantic-search-elser.asciidoc[]
include::cohere-es.asciidoc[]
include::semantic-search-deploy-model.asciidoc[]
