[[semantic-reranking]]
=== Semantic reranking

Reranking improves the relevance of results from any retrieval mechanism, such as lexical/BM25, semantic, or hybrid strategies. Reranking usually operates on a small subset of _top-k_ results (think 10-50 documents) using more computationally expensive techniques.

First-stage retrievers and rankers must be very fast and efficient because they process either the entire corpus, or all matching documents. In a multi-stage pipeline, you can progressively use more expensive ranking functions and techniques, as they will operate on smaller top-k result sets at each step.

_Semantic_ reranking models reorder search results based on their semantic similarity to a query.
Semantic rerankers don't care about the retrieval algorithm used to return the top-k documents: they work equally well with BM25, semantic, or hybrid search algorithms.
Because semantic reranking happens at query time, it's more computationally intensive.
Operating on small result sets helps avoid query latency degradation.

In Elasticsearch, a reranker is a special retriever that combines a “sub-retriever” and reranking configuration. The retriever syntax makes it simple to define your retrieval pipeline under your chosen reranking task with a single API call.

To use semantic reranking in Elasticsearch, you need to:

* Choose a reranking model. Upload a model using Eland, or integrate with models provided by third-party services such as OpenAI, Cohere, and Hugging Face.
* Create a `rerank` task using the <<put-inference-api,{es} Inference API>>.
The Inference API creates an inference endpoint and configures your chosen machine learning model to perform the reranking task.

==== Benefits

Semantic reranking enables many use cases, including:

* Out of the box semantic search for lexical/BM25 result sets by adding one API call
* Semantic search without reindexing
* Improved semantic search from semantic retrievers, dense or sparse (ELSER)
* Automatic and transparent chunking support, with no need to pre-chunk at index time
* The ability to explicitly control relevance for RAG/LLM contexts, easily cutting off irrelevant documents

==== Semantic reranking modes

Semantic reranking has two primary modes:

* Reranking *lexical* (BM25) results. A simple, *query time only* improvement that provides semantic search capabilities on top of any existing index. It requires no reindexing and is therefore easy to test out. This is good if you have a complex set of existing indices.
* Reranking *semantic* retrieval pipeline results. These could be ELSER sparse vector embeddings, dense vector embeddings, or hybrid search pipelines.

==== Cross-encoder and bi-encoder models

At a high level, two model types are used for semantic reranking: cross-encoders and bi-encoders. Both model types can be used to determine the similarity between the query and document, but they operate at different levels and with different inputs.

* A *cross-encoder model* can be thought of as an all-in-one solution.
It takes the query and document texts as a single, concatenated input.
It outputs a classification score indicating the document's relevance to the query.
This score is used for ranking.
** It is computationally infeasible to compute query-aware document representations for every single query-document pair, for every new query.
* A *bi-encoder model* takes as input either document or query text.
It outputs an embedding representing the text.
Documents and query representations are computed separately, so they aren't aware of each other.
Pre-computing document representations is computationally efficient because additional inference is only required to encode queries.
** To compute a ranking score, an external operation is required.
Typically, this involves calculating the dot-product or cosine similarity between the query and document embeddings.
This similarity score can then be used for ranking.

In addition to cross-encoder and bi-encoder models running on Elasticsearch Inference, we also expose external models and services via the Inference API to semantic rerankers.
This includes cross-encoder models running in https://huggingface.co/inference-endpoints[HuggingFace Inference Endpoints] and the https://cohere.com/rerank[Cohere Rerank API], in addition to any available bi-encoders already exposed for text embedding tasks.
If you're interested in more details about the differences between cross-encoders and bi-encoders, untoggle the collapsible below.

.Comparisons between cross-encoder and bi-encoder
[%collapsible]
==============
The following is a non-exhaustive list of considerations when choosing between cross-encoders and bi-encoders for semantic reranking:

* Because a cross-encoder model simultaneously processes both query and document texts, it can better infer their relevance, making it more effective as a reranker than a bi-encoder.
* Cross-encoder models are generally larger and more computationally intensive, resulting in higher latencies and increased computational costs.
* There are significantly fewer open-source cross-encoders, while bi-encoders offer a wide variety of sizes, languages, and other trade-offs.
* The effectiveness of cross-encoders can also improve the relevance of semantic retrievers.
For example, their ability to take word order into account can improve on dense or sparse embedding retrieval.
* When trained in tandem with specific retrievers (like lexical/BM25), cross-encoders can “correct” typical errors made by those retrievers.
* Cross-encoders output scores that are consistent across queries.
This means enables you to maintain high relevance in result sets, by setting a minimum score threshold for all queries.
For example, this is important when using results in a RAG workflow or if you're otherwise feeding results to LLMs.
Note that similarity scores from bi-encoders/embedding similarities are _query-dependent_, meaning you cannot set universal cut-offs.
* Bi-encoders rerank using embeddings. Improve your reranking latency by creating embeddings at ingest-time. These embeddings can be stored for reranking without being indexed for retrieval, reducing your memory footprint.
==============

==== Supported reranking types

===== Text similarity with cross-encoder

This solution uses a hosted or 3rd party inference service which relies on a cross-encoder model.
The model receives the text fields from the top-K documents, as well as the search query, and calculates scores directly, which are then used to rerank the documents.

Used with the Cohere inference service rolled out in 8.13, turn on semantic reranking that works out of the box.

DEMO: You can find a working demo of hybrid search and reranking with Cohere in link:https://github.com/elastic/elasticsearch-labs/blob/demjened/cohere-reranking/notebooks/integrations/cohere/cohere-reranking.ipynb[this Python notebook].

===== Text similarity with bi-encoder

Similarly to the cross-encoder approach, the inference service is invoked on both the text fields and the search query.
The generated vector embeddings are then compared with vector similarity, in order to produce the scores.

===== Vector similarity

This approach assumes vectors are generated at ingestion time and stored in the documents.
At query time the same bi-encoder model is used to generate embeddings for the search query, after which vector similarity is calculated.

This is faster and cheaper due to front-loading of the bulk of the costly processing at ingest time.
