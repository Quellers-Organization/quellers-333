---
"Custom normalizer with illegal filter in request":
    # Tests analyze api with normalizer. This is in the analysis-common module
    # because there are no filters that support multiTermAware
    - do:
        catch: bad_request
        indices.analyze:
          body:
            text: ABc
            explain: true
            filter: [word_delimiter]

    - match: { status: 400 }
    - match: { error.type: illegal_argument_exception }
    - match: { error.reason: "Custom normalizer may not use filter [word_delimiter]" }

---
"Synonym filter with tokenizer":
    - do:
        indices.create:
          index: test_synonym
          body:
            settings:
              index:
                analysis:
                  tokenizer:
                    trigram:
                      type: ngram
                      min_gram: 3
                      max_gram: 3
                  filter:
                    synonym:
                      type: synonym
                      synonyms: ["kimchy => shay"]

    - do:
        indices.analyze:
          index: test_synonym
          body:
            tokenizer: trigram
            filter: [synonym]
            text: kimchy
    - length: { tokens: 2 }
    - match:  { tokens.0.token: sha }
    - match:  { tokens.1.token: hay }

---
"Too complex regex pattern":
    - do:
        catch: bad_request
        indices.create:
          index: test_too_complex_regex_pattern
          body:
            settings:
              index:
                analysis:
                  analyzer:
                    my_analyzer:
                      tokenizer: standard
                      char_filter:
                        - my_char_filter
                  char_filter:
                    my_char_filter:
                      type: "pattern_replace"
                      pattern: "(\\d+)-(?=\\d\nͭͭͭͭͭͭͭͭͭͭͭͭͭͭͭ"
                      flags: CASE_INSENSITIVE|MULTILINE|DOTALL|UNICODE_CASE|CANON_EQ
                      replacement: "_$1"
    - match: { status: 400 }
    - match: { error.type: illegal_argument_exception }
    - match: { error.reason: "Too complex regex pattern" }


---
"Custom normalizer in request":
    - do:
        indices.analyze:
          body:
            text: ABc
            explain: true
            filter: ["lowercase"]

    - length: { detail.tokenizer.tokens: 1 }
    - length: { detail.tokenfilters.0.tokens: 1 }
    - match:  { detail.tokenizer.name: keyword }
    - match:  { detail.tokenizer.tokens.0.token: ABc }
    - match:  { detail.tokenfilters.0.name: lowercase }
    - match:  { detail.tokenfilters.0.tokens.0.token: abc }
